# WBSÂ Classifier

A **Workâ€‘Breakdownâ€‘Structure (WBS) text classifier** that sorts costâ€‘line descriptions into one of eight highâ€‘level categories:

| ID | Category                                 |
| -- | ---------------------------------------- |
| 0  | Design and NRE                           |
| 1  | OPCÂ Testing & Startup                    |
| 2  | Construction                             |
| 3  | ProcessÂ Equipment                        |
| 4  | D\&DÂ (Decontamination & Decommissioning) |
| 5  | SEPMÂ (System Engineering & ProjectÂ Mgmt) |
| 6  | OtherÂ Labor & Support                    |
| 7  | StandardÂ Equipment                       |

Built with **PyTorchÂ 2.2**, the project provides an endâ€‘toâ€‘end pipelineâ€”**preâ€‘processing â†’ training â†’ evaluation â†’ inference**â€”and ships three readyâ€‘made neural architectures (Textâ€‘CNN, GRU, LSTM).  All paths and hyperâ€‘parameters are centrally managed in `src/config.py` so you can experiment without touching the plumbing.

---

## ðŸ“‘Â Table of Contents

1. [Installation](#installation)
2. [QuickÂ Start](#quick-start)
3. [ProjectÂ Layout](#project-layout)
4. [Configuration](#configuration)
5. [ExtendingÂ / Contributing](#extending--contributing)
6. [TroubleshootingÂ &Â FAQ](#troubleshooting--faq)

---

## Installation

```bash
# 1Â â–ªÂ Clone the repo
$ git clone https://github.com/yourâ€‘org/wbs_classifier.git
$ cd wbs_classifier

# 2Â â–ªÂ (Recommended) create a fresh virtualÂ env
$ python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 3Â â–ªÂ Install Python dependencies
$ pip install -r requirements.txt
```

The project has been tested on **PythonÂ 3.10+** with both CPU and CUDAÂ 11.8 wheels of PyTorch.

---

## QuickÂ Start

Below is the minimal happyâ€‘path flow. All commands are run from the repository root.

### 1Â â–ªÂ Preâ€‘process raw data

Converts your raw CSV into token tensors, vocabulary, and train/val/test splits.

```bash
$ python -m src.data.preprocess \
        --csv data/raw/wbs_data.csv  # adjust path if needed
```

Generated artefacts land in `data/processed/` and are *gitâ€‘ignored*.

### 2Â â–ªÂ Train a model

```bash
$ python -m src.train                          # uses hyperâ€‘params in config.py
# or override on the CLI
$ python -m src.train --model_arch text_cnn
```

The best checkpoint is saved to **`best_model.pth`** with earlyâ€‘stopping.

### 3Â â–ªÂ Evaluate

```bash
$ python -m src.evaluate --split val --outdir reports/val_eval
```

Youâ€™ll get:

* `predictions.csv` â€“ groundâ€‘truth vs prediction per row
* `classification_report.json` â€“ precision / recall / F1 per class
* `confusion_matrix.png`, `training_curves.png`, etc.

### 4Â â–ªÂ RunÂ inference

```bash
$ python -m src.predict data/raw/new_wbs.csv
# â†’ data/raw/new_wbs_predictions.csv
```

`new_wbs.csv` must contain a `wbs_name` column.

---

## ProjectÂ Layout

```
wbs_classifier/
â”œâ”€â”€ data/                   # raw & processed data (gitâ€‘ignored except tiny samples)
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ exploratory.ipynb   # Jupyter exploration
â”œâ”€â”€ reports/                # evaluation artefacts & plots
â”œâ”€â”€ src/                    # all Python source code
â”‚   â”œâ”€â”€ data/               # preprocess.py, dataset.py, collate helpers
â”‚   â”œâ”€â”€ models/             # text_cnn.py, gru_rnn.py, lstm_rnn.py, registry
â”‚   â””â”€â”€ utils.py            # misc helpers (metrics, tokeniser, plotting)       
â”œâ”€â”€ best_model.pth          # latest/best checkpoint (ignored by Git LFS)
â”œâ”€â”€ README.md               # latest/best checkpoint (ignored by Git LFS)
â””â”€â”€ Requirements.txt        # package versions
```

> **Tip:** Every script carries a detailed docâ€‘string and can be invoked with `â€‘h` for help.

---

## Configuration

All knobs live in **`src/config.py`**:

```python
class Config:
    MODEL_ARCH = "lstm_rnn"      # text_cnn | gru_rnn | lstm_rnn
    SEQ_LEN    = 64              # tokens per sample
    EMBED_DIM  = 128
    HIDDEN_DIM = 256
    LR         = 1eâ€‘4
    EPOCHS     = 15
    BATCH_SIZE = 128
    DEVICE     = "cuda"          # "cpu" forces CPU, "auto" picks best
    ...
```

Changes are *singleâ€‘sourceâ€‘ofâ€‘truth*:Â every script imports `Config` so you never duplicate paths or magic numbers.

---

## ExtendingÂ / Contributing

* **Add a new model** â†’ drop a module in `src/models/` and register it in `src/models/__init__.py` (see comments there).
* **Tune hyperâ€‘parameters** â†’ edit `config.py` or pass `--flag value` on the CLI.
* **Speedâ€‘ups** â†’ enable mixedâ€‘precision, batchâ€‘size sweeps, or multiâ€‘GPU training (script skeletons provided in `scripts/experimental/`).

Please open a PR or issue if you hit problemsâ€”the maintainers welcome improvements!

---

## TroubleshootingÂ &Â FAQ

| Symptom                             | Fix                                                                                                             |
| ----------------------------------- | --------------------------------------------------------------------------------------------------------------- |
| *Training is stuck on CPU*          | Make sure you installed the **CUDA** wheel of PyTorch (`+cu118`) and that `Config.DEVICE` is set to `"cuda"`.   |
| *`FileNotFoundError: wbs_data.csv`* | Place your data under `data/raw/` or pass `--csv path/to/file`.                                                 |
| *Low F1 score*                      | Check class imbalance in `reports/eval/data_distribution.csv`; try `--model_arch gru_rnn` or additional epochs. |

---

Â©Â 2025Â DepartmentÂ ofÂ EnergyÂ â€“Â Internal prototype. Use at your own risk.
